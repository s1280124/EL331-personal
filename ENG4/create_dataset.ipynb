{"cells":[{"cell_type":"markdown","metadata":{"id":"S3bxLVKDJFPz"},"source":["# Machine Learning for Author Attribution - Create Dataset\n","\n","## Genevieve Hayes\n","### 10th November 2018"]},{"cell_type":"code","source":["# Add: by s1280124\n","# Google Colaboratoryで動かしているので、Google Driveのアクセスを許可させる\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# このプロジェクトの為のディレクトリまで移動する\n","# 使う人や環境によって変更してください\n","import os\n","os.chdir('/content/drive/MyDrive/Colab/ENG4')\n","print(f'Move to:　{os.getcwd()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhB3sSlHJXzQ","executionInfo":{"status":"ok","timestamp":1683015082121,"user_tz":-540,"elapsed":18123,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}},"outputId":"e0481fbb-83ee-4551-a203-a991c8626fcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Move to:　/content/drive/MyDrive/Colab/ENG4\n"]}]},{"cell_type":"markdown","metadata":{"id":"TKbBOLOAJFP1"},"source":["### Overview\n"]},{"cell_type":"markdown","metadata":{"id":"lYEg8CCDJFP1"},"source":["This notebook is used to create the dataset used in the Analysis notebook. The final output is a csv file containing sentence-long text excerpts from the works of famous authors, along with labels identifying the authors of the excerpts. \n","\n","To create this dataset, we use novels written by eight classic authors (Louisa May Alcott, Jane Austen, Charlotte Bronte, Wilkie Collins, Arthur Conan Doyle, L.M. Montgomery, Bram Stoker and Mark Twain), all of whom wrote in the English language during the 19th and early-20th century. The novel texts were obtained as text files from [Project Gutenburg](https://www.gutenberg.org/) and chapter/section headings were manually removed from the files prior to processing, since these were not considered to be part of the main text.\n","\n","To allow for the creation of a balanced dataset, for authors whose novels tended to be shorter in length, text excerpts were taken from multiple works.\n","\n","The novels used to create the dataset are as follows:\n"]},{"cell_type":"markdown","metadata":{"id":"o3M98Z7TJFP1"},"source":["\n","|Author     | Novels| Genre | Year of Publication|\n","|---------  |-------|-------|--------------------|\n","|Louisa May Alcott | *Little Women* |Coming of Age/Romance | 1869 |\n","|Jane Austen| *Pride and Prejudice* and *Emma*|Romance | 1813/1815 |\n","|Charlotte Bronte| *Jane Eyre* | Gothic Romance | 1847 |\n","|Wilkie Collins | *The Woman in White* | Mystery | 1859 |\n","|Arthur Conan Doyle | *A Study in Scarlet*, *The Sign of the Four* and *The Hound of the Baskervilles*| Mystery |1887/1890/1902| \n","|L.M. Montgomery | *Anne of Green Gables* and *Anne of Avonlea* |Coming of Age | 1908/1909 |\n","|Bram Stoker | *Dracula* | Horror | 1897|\n","|Mark Twain | *The Adventures of Tom Sawyer* and *The Adventures of Huckleberry Finn*|Coming of Age/Adventure|1876/1884|"]},{"cell_type":"markdown","metadata":{"id":"fmb-zZ1iJFP2"},"source":["### Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2X_ZqfzsJFP2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683015104358,"user_tz":-540,"elapsed":247,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}},"outputId":"2ea3999d-a106-48b3-8fdd-5c39decaefce"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["from nltk import tokenize\n","import numpy as np\n","import random\n","import pandas as pd\n","\n","# これが必要\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"ebZ7M7EMJFP3"},"source":["### Load Data and Create Combined Dataset"]},{"cell_type":"markdown","metadata":{"id":"iGQidxbSJFP3"},"source":["In creating the sentence lists, we exclude sentences of less than 5 characters in length, as these are unlikely to be proper sentences and are likely too short to contain any useful information. As the sentence tokenizer has difficulties in identifying the end of sentences under some circumstances (e.g. if the full-stop at the end of the sentence is contained within quotation marks), we make some minor adjustments to the text prior to tokenization using the `replace` function."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MCrvt3nEJFP4"},"outputs":[],"source":["def split_text(filepath, min_char):\n","    \"\"\"Convert text file to a list of sentences.\n","    \n","    Args:\n","    filepath: string. Filepath of text file.\n","    min_char: int. Minimum number of characters required for a sentence to be\n","    included.\n","\n","    Returns:\n","    sentences: list of strings. List of sentences containined in the text file.\n","    \"\"\"\n","    # Load data into string variable and remove new line characters\n","    file = open(filepath, \"r\", encoding=\"utf8\")\n","    text = file.read().replace('\\n', ' ')\n","    text = text.replace('.”', '”.').replace('.\"', '\".').replace('?”', '”?').replace('!”', '”!')\n","    text = text.replace('--', ' ').replace('. . .', '').replace('_', '')\n","    file.close()\n","    \n","    # Split text into a list of sentences\n","    sentences = tokenize.sent_tokenize(text)\n","    \n","    # Remove sentences that are less than min_char long\n","    sentences = [sent for sent in sentences if len(sent) >= min_char]\n","\n","    return list(sentences)"]},{"cell_type":"markdown","metadata":{"id":"aCI6vwLOJFP4"},"source":["**Create sentence list for each author**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"T5Q4kUYsJFP5"},"outputs":[],"source":["# Set parameter values\n","min_char = 5\n","\n","# Create lists\n","alcott = split_text('Books/Little_Women.txt', min_char = min_char)\n","austen = split_text('Books/Pride_and_Prejudice.txt', min_char = min_char)\\\n","         + split_text('Books/Emma.txt', min_char = min_char)\n","bronte = split_text('Books/Jane_Eyre.txt', min_char = min_char)\n","collins = split_text('Books/Woman_in_White.txt', min_char = min_char)\n","doyle = split_text('Books/Study_in_Scarlet.txt', min_char = min_char)\\\n","        + split_text('Books/Sign_of_the_Four.txt', min_char = min_char)\\\n","        + split_text('Books/Hound_of_the_Baskervilles.txt', min_char = min_char)\n","montgomery = split_text('Books/Anne_of_Green_Gables.txt', min_char = min_char)\\\n","             + split_text('Books/Anne_of_Avonlea.txt', min_char = min_char)\n","stoker = split_text('Books/Dracula.txt', min_char = min_char)\n","twain = split_text('Books/Tom_Sawyer.txt', min_char = min_char)\\\n","        + split_text('Books/Huckleberry_Finn.txt', min_char = min_char)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZksSMpp0JFP5","outputId":"65d9c9cc-7748-4112-b59e-d4209365d4c3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683015116012,"user_tz":-540,"elapsed":268,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Alcott : 9447\n","Austen : 14414\n","Bronte : 9767\n","Collins : 13520\n","Doyle : 9421\n","Montgomery : 12274\n","Stoker : 8641\n","Twain : 10712\n"]}],"source":["# Print length of each list\n","\n","text_dict = {'Alcott': alcott, 'Austen': austen, 'Bronte': bronte, 'Collins': collins,\n","             'Doyle': doyle, 'Montgomery': montgomery, 'Stoker': stoker, 'Twain': twain}\n","\n","for key in text_dict.keys():\n","    print(key, ':', len(text_dict[key]))"]},{"cell_type":"markdown","metadata":{"id":"JE4qzpoKJFP6"},"source":["All lists contain between 8641 and 14414 sentences. So that our final dataset doesn't become skewed towards a single author, we will randomly select 8500 sentences from each list (without replacement) to form the final dataset."]},{"cell_type":"markdown","metadata":{"id":"dhQdDZDmJFP6"},"source":["**Select and combine sentences **"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uY88GCWFJFP6","outputId":"d19a128f-2f88-4f29-fb20-64ce818f59ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683015118973,"user_tz":-540,"elapsed":1012,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The length of the combined list is: 68000\n"]}],"source":["# Set random seed\n","np.random.seed(1)\n","\n","# Set length parameter\n","max_len = 8500\n","\n","# Select sentences\n","names = [alcott, austen, bronte, collins, doyle, montgomery, stoker, twain]\n","combined = []\n","\n","for name in names:\n","    name = np.random.choice(name, max_len, replace = False)\n","    combined += list(name)\n","\n","print('The length of the combined list is:', len(combined))"]},{"cell_type":"markdown","metadata":{"id":"Ytx3FWsuJFP6"},"source":["**Create labels list**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRlZKs45JFP7","outputId":"bb444d95-dbeb-476c-f491-872587301c37","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683015120852,"user_tz":-540,"elapsed":257,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The length of the labels list is: 68000\n"]}],"source":["labels = ['Alcott']*max_len + ['Austen']*max_len + ['Bronte']*max_len + ['Collins']*max_len\\\n","         + ['Doyle']*max_len + ['Montgomery']*max_len + ['Stoker']*max_len + ['Twain']*max_len\n","\n","print('The length of the labels list is:', len(labels))"]},{"cell_type":"markdown","metadata":{"id":"NveSt_BSJFP7"},"source":["**Randomly sort data**"]},{"cell_type":"markdown","metadata":{"id":"2ugSpvs3JFP7"},"source":["We randomly shuffle the data to avoid any issues arising from the bunching together of sentences by a single author."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-yko302cJFP7"},"outputs":[],"source":["# Set random seed\n","random.seed(3)\n","\n","# Randomly shuffle data\n","zipped = list(zip(combined, labels))\n","random.shuffle(zipped)\n","combined, labels = zip(*zipped)"]},{"cell_type":"markdown","metadata":{"id":"4BK8bZOPJFP7"},"source":["**Create and export final dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ze-Uoy15JFP7","outputId":"9e1ed029-ceee-4788-94d2-71be606cadd0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683015124165,"user_tz":-540,"elapsed":263,"user":{"displayName":"尾形駿樹","userId":"11152845971459940871"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                text   author\n","0  I'm afraid I couldn't like him without a spice...   Alcott\n","1  Yonder was the banks and the islands, across t...    Twain\n","2  Well, as I was saying about the parlor, there ...    Twain\n","3  Here, again, the Count had not openly committe...  Collins\n","4  “No,” assented Tom, “they don't kill the women...    Twain\n"]}],"source":["# Create pandas dataframe\n","out_data = pd.DataFrame()\n","out_data['text'] = combined\n","out_data['author'] = labels\n","\n","print(out_data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BJYraKQwJFP8"},"outputs":[],"source":["# Export as a csv file\n","out_data.to_csv('./data/author_data.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}